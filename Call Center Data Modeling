import numpy as np
import scipy as sp
import scipy.stats as sts
import matplotlib.pyplot as plt

%matplotlib inline

# Load the data set containing durations between calls arriving at the call
# center during 1 day. All values are in minutes.
waiting_times_day = np.loadtxt('call_center.csv')

# Make 24 empty lists, one per hour.
waiting_times_per_hour = [[] for _ in range(24)]

# Split the data into 24 separate series, one for each hour of the day.
current_time = 0
for t in waiting_times_day:
    current_hour = int(current_time // 60)
    current_time += t
    waiting_times_per_hour[current_hour].append(t)
# Exponential distribution with maximum likelihood fit to the data

# THE LIKELIHOOD FUNCTION
#
# This function takes 2 inputs, namely the parameter (λ) value at which to
# compute the likelihood and a value from the data set. You need to evaluate
# the exponential distribution of the datum given the parameter value.

def likelihood(lambda_, datum):
    return sts.expon(scale=1/lambda_).pdf(datum)

# THE PRIOR DISTRIBUTION
#
# This function takes 1 input, namely the parameter value (λ) at which to
# compute the prior probability density. You need to evaluate the distribution
# Gamma(λ | α, β).

def prior(lambda_):
    return sts.gamma(1, scale=1/0.25).pdf(lambda_)

# THE POSTERIOR DISTRIBUTION
#
# The function below is provided to help make computing the posterior easier. It
# follows the same pattern as in the previous class, where we multiple the prior
# and the likelihood evaluated at various values of the parameter (λ).
#
# You will see the function uses the logarithms of various distributions. This
# is for numerical reasons. When you multiply lots of likelihood values, the
# result can become very large or very small, causing numerical overflow or
# underflow in Python. Taking logarithms avoids this problem.

def compute_posterior(parameter_values, prior, likelihood, data):
    log_prior = np.log(prior(parameter_values))
    log_likelihood = np.array([
        np.sum(np.log(likelihood(param, data)))
        for param in parameter_values])
    unnormalized_log_posterior = log_prior + log_likelihood
    unnormalized_log_posterior -= max(unnormalized_log_posterior)
    unnormalized_posterior = np.exp(unnormalized_log_posterior)
    area = sp.integrate.trapz(unnormalized_posterior, parameter_values)
    posterior = unnormalized_posterior / area
    return posterior

def compute_percentile(parameter_values, distribution_values, percentile):
    '''
    Compute the parameter value at a particular percentile of the given
    probability distribution values. This function uses the cumulative trapezoid
    integrator in SciPy.

    Inputs:

        parameter_values (array of float) This is the list of parameter values
          at which the probability distribution has been evaluated.

        distribution_values (array of float) This is the list of values of the
          probability density function evaluated at the parameter values above.

        percentile (float) This is the value between 0 and 1 of the percentile
          to compute.

    Returns: (float) The parameter value at the given percentile.
    '''
    cumulative_distribution = sp.integrate.cumtrapz(
        distribution_values, parameter_values)
    percentile_index = np.searchsorted(cumulative_distribution, percentile)
    return lambdas[percentile_index]

#empty lists to store the mean values for each hour and the confidence intervals
x=[]
y=[]
dy=[]
for i in range(0, 23):
    hour_index = i

    waiting_times_hour = waiting_times_per_hour[hour_index]
    
    lambda_ = 1 / np.mean(waiting_times_hour)
    distribution = sts.expon(scale=1/lambda_)

 
    lambdas = np.linspace(0, 10, 501)[1:]
    posterior = compute_posterior(lambdas, prior, likelihood, waiting_times_hour)

    # Calculates the confidence interval for 98%
    percentiles = [compute_percentile(lambdas, posterior, p) for p in [0.01, 0.99]]
    print('Posterior 98% interval for',hour_index,":", percentiles, 'calls per minute')
    # Calculates the expected value by using the likelihood and the prior
    lambda_ = sp.integrate.trapz(posterior * lambdas, lambdas)
    x.append(hour_index)
    y.append(lambda_)
    (percentiles[1]-percentiles[0])/2
    dy.append((percentiles[1]-percentiles[0])/2)
    print('Expected value: λ =', lambda_, 'calls per minute')
    
#if we want to graph the individual pdf graphs for each hour we can use the following code.
"""plt.figure(figsize=(8, 6))
    plt.plot(lambdas, posterior, label='posterior')
    plt.axvline(percentiles[0], color='red', linestyle=':', label='98% interval')
    plt.axvline(percentiles[1], color='red', linestyle=':')
    plt.xlabel('λ [calls per minute]')
    plt.ylabel('probability density')
    plt.title(f'Inference results for hour {hour_index}')
    plt.legend()
    plt.show()"""

#Plot for the means and 98% confidence interval
plt.figure(figsize=(18, 10))
plt.errorbar(x, y, yerr=dy, fmt='o')
plt.xlabel('Hour of the day')
plt.ylabel('Call rate (calls per minute)')
plt.title('Mean and 98% confidence interval over call rate')
plt.errorbar(x, y, yerr=dy, fmt='o', color ="blue")
plt.show()


# Plot the number of calls per hour using a bar chart to help the client see the over all 
plt.figure(figsize=(8, 6))
plt.bar(range(24), [len(w) for w in waiting_times_per_hour])
plt.xlabel('Hour of the day')
plt.ylabel('Number of calls')
plt.title('Number of calls per hour in the data set')
plt.show()

